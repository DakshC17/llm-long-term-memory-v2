{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152fc514",
   "metadata": {},
   "source": [
    "## Ollama Integration\n",
    "\n",
    "This notebook focuses on integrating a local LLM (Ollama) with our memory system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330dcacf",
   "metadata": {},
   "source": [
    "## Step 1: Install Ollama\n",
    "\n",
    "Before we proceed, you need to install Ollama on your system.\n",
    "\n",
    "### Installation Command:\n",
    "```bash\n",
    "curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "\n",
    "For Linux/WSL\n",
    "wget -qO- https://ollama.ai/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d979db",
   "metadata": {},
   "source": [
    "Now verify the installation\n",
    "```bash\n",
    "ollama --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57675d6",
   "metadata": {},
   "source": [
    "## Step 2: Install Ollama Python Client\n",
    "\n",
    "Now we will install the Python client library to communicate with Ollama from our code.\n",
    "\n",
    "### Installation Command:\n",
    "```bash\n",
    "pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796c5ed",
   "metadata": {},
   "source": [
    "## Step 3: Download LLM Model\n",
    "\n",
    "Now let's download a language model to use with our memory system.\n",
    "\n",
    "### Recommended Model (Lightweight):\n",
    "```bash\n",
    "ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe37f4",
   "metadata": {},
   "source": [
    "### Verify Model Installation\n",
    "\n",
    "``` bash\n",
    "ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14330e33",
   "metadata": {},
   "source": [
    "### Test the model now\n",
    "``` bash \n",
    "ollama run llama3.2:3b\n",
    "# Type a test message and exit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d3c651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Llama 3.2 model...\n",
      "Model is working!\n",
      "========================================================================================================================================================================================================\n",
      "Response: Hello! I'm an artificial intelligence model, which means I'm a computer program designed to simulate conversations and answer questions to the best of my knowledge. I don't have a personal identity or physical presence, but I'm here to help and provide information on a wide range of topics.\n",
      "\n",
      "I can assist with tasks such as:\n",
      "\n",
      "* Answering questions\n",
      "* Generating text\n",
      "* Translating languages\n",
      "* Summarizing content\n",
      "* And more!\n",
      "\n",
      "Feel free to ask me anything, and I'll do my best to help. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "## We will just test the model with a small snippet,,\n",
    "\n",
    "import ollama\n",
    "\n",
    "print(\"Testing Llama 3.2 model...\")\n",
    "\n",
    "try:\n",
    "    # Simple test conversation for checking..\n",
    "    response = ollama.chat(\n",
    "        model='llama3.2:3b',\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Hello! who are you?'\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Model is working!\")\n",
    "    print(\"=\"*200)\n",
    "    print(f\"Response: {response['message']['content']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070ff75",
   "metadata": {},
   "source": [
    "## Step 4: Connect Ollama with Memory Database\n",
    "\n",
    "Now we'll integrate our LLM (Ollama) with the memory system we built in setup_vectorydb notebook.\n",
    "\n",
    "### What we're going to do:\n",
    "1. **Import ChromaDB** - Same as setup_vectordb notebook to access our memory\n",
    "2. **Connect to existing collection** - Get our stored conversations\n",
    "3. **Create memory-aware function** - Combine memory search + LLM response\n",
    "4. **Test the integration** - See how LLM uses past conversations\n",
    "\n",
    "### Below is the code for doing the above things (Most of the things we did earlier in setup_vectordb notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8020afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting Ollama with Memory Database...\n",
      "✅ Connected to existing memory collection!\n",
      " Total memories available: 10\n",
      " Collection name: conversation_memory\n",
      "\n",
      "====================================================================================================\n",
      " Memory-LLM Integration Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 4A: Import required libraries\n",
    "import chromadb\n",
    "import ollama\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"Connecting Ollama with Memory Database...\")\n",
    "\n",
    "# Step 4B: Connect to existing persistent ChromaDB from Phase 1\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Get the existing collection created in Phase 1\n",
    "collection = client.get_collection(name=\"conversation_memory\")\n",
    "\n",
    "print(\"✅ Connected to existing memory collection!\")\n",
    "print(f\" Total memories available: {collection.count()}\")\n",
    "print(f\" Collection name: {collection.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\" Memory-LLM Integration Setup Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7463f646",
   "metadata": {},
   "source": [
    "## Create Memory-Aware Chat Function\n",
    "### What we will buil now is:\n",
    "\n",
    "1. **Search function** - Find relevant past conversations\n",
    "2. **Context builder** - Format memory for LLM prompt\n",
    "3. **Memory aware chat** - LLM that uses conversation history\n",
    "4. **Test it** - See how memory improves responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea1ec54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Memory aware chat function created!\n"
     ]
    }
   ],
   "source": [
    "def search_memory(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for relevant past conversations based on the query\n",
    "    \"\"\"\n",
    "    print(f\" Searching memory for: '{query}'\")\n",
    "    \n",
    "    # Search for similar conversations\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    print(f\" Found {len(results['documents'][0])} relevant memories\")\n",
    "    return results\n",
    "\n",
    "def format_memory_context(search_results):\n",
    "    \"\"\"\n",
    "    Format search results into context for the LLM\n",
    "    \"\"\"\n",
    "    if not search_results['documents'][0]:\n",
    "        return \"No relevant conversation history found.\"\n",
    "    \n",
    "    context = \"=== RELEVANT CONVERSATION HISTORY ===\\n\\n\"\n",
    "    \n",
    "    for i, (doc, metadata) in enumerate(zip(\n",
    "        search_results['documents'][0], \n",
    "        search_results['metadatas'][0]\n",
    "    )):\n",
    "        context += f\"Memory {i+1}:\\n\"\n",
    "        context += f\"Content: {doc}\\n\"\n",
    "        context += f\"Topic: {metadata.get('topic', 'Unknown')}\\n\"\n",
    "        context += f\"User Skill Level: {metadata.get('user_level', 'Unknown')}\\n\"\n",
    "        context += f\"Technology: {metadata.get('tech_stack', 'Unknown')}\\n\"\n",
    "        context += f\"Timestamp: {metadata.get('timestamp', 'Unknown')}\\n\"\n",
    "        context += \"-\" * 50 + \"\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "def memory_chat(user_message, use_memory=True):\n",
    "    \"\"\"\n",
    "    Chat with LLM using conversation memory for context\n",
    "    \"\"\"\n",
    "    print(\"=\"*100)\n",
    "    print(f\" User: {user_message}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # now we will build a very detailed and contextual  prompt\n",
    "    if use_memory:\n",
    "        # Searching  for relevant memorie\n",
    "        memory_results = search_memory(user_message)\n",
    "        memory_context = format_memory_context(memory_results)\n",
    "        \n",
    "        # what i have done is that i have enhanced professional system prompt with memory\n",
    "        system_prompt = f\"\"\"You are an advanced AI assistant with persistent memory capabilities. Your primary objective is to provide highly personalized, contextually-aware responses based on the user's conversation history and demonstrated preferences.\n",
    "\n",
    "{memory_context}\n",
    "\n",
    "CORE INSTRUCTIONS:\n",
    "1. MEMORY UTILIZATION: Always reference and build upon the provided conversation history to demonstrate continuity and understanding of the user's background, interests, and expertise levels.\n",
    "\n",
    "2. PERSONALIZATION: Adapt your communication style, technical depth, and examples to match the user's demonstrated skill level and interests from previous conversations.\n",
    "\n",
    "3. CONTEXTUAL AWARENESS: Connect current questions to past discussions when relevant, showing how topics relate to the user's ongoing projects or learning journey.\n",
    "\n",
    "4. CONSISTENCY: Maintain awareness of the user's preferences, previously discussed technologies, and established context across all interactions.\n",
    "\n",
    "5. PROGRESSIVE LEARNING: Build upon previous conversations to offer increasingly sophisticated insights and recommendations that align with the user's growing expertise.\n",
    "\n",
    "6. RELEVANCE FILTERING: Only reference historical context that is directly relevant to the current query - avoid overwhelming responses with unnecessary background information.\n",
    "\n",
    "7. ACKNOWLEDGE LIMITATIONS: If the conversation history doesn't contain relevant information for the current query, explicitly state this and provide the best possible response based on available context.\n",
    "\n",
    "RESPONSE REQUIREMENTS:\n",
    "- Be specific and actionable\n",
    "- Reference relevant past discussions naturally  \n",
    "- Maintain professional yet personable tone\n",
    "- Provide depth appropriate to user's demonstrated skill level\n",
    "- Offer practical next steps when applicable\n",
    "- Use examples that align with user's known interests and experience level\n",
    "\n",
    "TECHNICAL CONTEXT AWARENESS:\n",
    "- Consider the user's demonstrated expertise with specific technologies\n",
    "- Build upon their known project context and learning goals\n",
    "- Suggest resources appropriate to their skill progression\n",
    "- Connect new concepts to their established knowledge base\n",
    "\n",
    "Respond to the user's current message while seamlessly integrating insights from their conversation history to provide the most helpful and contextually appropriate response possible.\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful AI assistant. Provide clear, accurate, and contextually appropriate responses to user queries. Be professional, concise, and ensure your answers are actionable and well-structured.\"\"\"\n",
    "    \n",
    "    # Chat with Ollama\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3.2:3b',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_prompt},\n",
    "                {'role': 'user', 'content': user_message}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        ai_response = response['message']['content']\n",
    "        print(f\" AI Response:\\n{ai_response}\")\n",
    "        \n",
    "        return ai_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Memory aware chat function created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42d10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
